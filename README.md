# Smart Containers

[![Join the chat at https://gitter.im/charlesvardeman/sc_spec](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/charlesvardeman/sc_spec)

## Table of contents

 1. [Quick intro](#quick-intro)
 2. [Brief example of Smart Containers in action](#brief-example-of-solid-in-action)
 3. [RDF](#rdf)
 4. [Ontologies](#Ontologies)
 4. [Reading and writing data using LDP](#reading-and-writing-data-using-ldp)
 5. [Reading and writing data using SPARQL](#reading-and-writing-data-using-sparql)
 8. [Identity management](#identity-management-based-on-webid)
 9. [Personal data workspaces](#personal-data-workspaces)
 10. [Authentication](#authentication)
 11. [Access control](#access-control)
 12. [Software implementing SmartContainers](#software-implementing-solid)

Specification for Smart Containers

## Quick Intro
Scientists need an efficient approach to preserve and share the results of computational work and in fact, is required by reproducibility requirements of the scientific process. Although a variety of technologies have been created to capture computational experiments, it is still a struggle for scientists to understand the preserved experiment without a detailed documentation and context of the results being shared. Smart Container (SC) is a modular specification and implementation that leverages linked data principles to enable the preservation, sharing and reuse of both software and data artifacts.

Smart Containers begin with conceptualizing computational experiments from the perspective of computational environments and activities within those environments as provided by the Docker Linux container framework. Docker is a lightweight virtualization platform that has several properties such as versioning of file system operations, a modular design for distribution of software components as well as a sustainable community that make it attractive as a preservation tool. One community of collaborators at CERN is already exploring Docker to preserve high energy physics experiments. Our approach is to eventually provide a mechanism that captures the additional provenance of computational experiments in a machine readable approach using the W3C standard RDF data model that has been shown to aid in contextualization of scientific experiment.

By starting with a formal model of Docker and the provenance of Docker activities, we hope to provide a basis for 1) existing scientific workflow frameworks and their workflow descriptions to be captured within a Linux container environment 2) data to be integrated in a consistent manner and lastly 3) for a common description of the environment. The ultimate goal being to provide automated tools that “wrap” the existing Docker command line tool and infrastructure such that it is transparent to the scientist but captures information necessary to populate the metadata behind the scenes. Automated scientific gateways that utilize Docker as a deployment and execution platform would provide a further degree of transparency and allow researchers a low barrier for utilization.

Features:


## Brief Example of SmartContainers in action


## Docker
[Docker](https://www.docker.com/) is an application that encapsulates and extends a Linux Kernel feature known as Linux Containers (LXC). It isolates an application with its dependencies in a single process which is more light-weighted than full hypervisor virtualization of guest OS. It can be provisioned by a simple Dockerfile text based workflow. Docker also adopted a layer file system way to achieve versioning and component re-use. A Docker image is a read-only layer which is stateless. A container has states: when it is running, it represents a tree of processes isolated from other processes on the host; when it exits, it represents a read-write layer generated by the process along with its all underneath stateless images. Docker provides the ability for the Docker community to share images through the [Docker Hub](https://hub.docker.com/) providing a degree of software and infrastructure reuse.

### General Docker Concepts
[The Docker User Guide](http://docs.docker.com/engine/userguide/) is a good resource for understanding how Docker works in action. Docker has created a distribution system for Docker Containers (LXC containers) that uses *Images* as the distribution mechanism. 
> "An Image is an ordered collection of root filesystem changes and the corresponding execution parameters for use within a container runtime. This specification outlines the format of these filesystem changes and corresponding parameters and describes how to create and use them for use with a container runtime and execution tool."

Docker composes images [(specification)](https://github.com/docker/docker/blob/master/image/spec/v1.md) out of file system layers, each of which represents a change to a base image. As a result, share images layers with many other repositories in the "Docker Hub". 
 > " **ImageID**
> Each layer is given an ID upon its creation. It is represented as a hexadecimal encoding of 256 bits, e.g., a9561eb1b190625c9adb5a9513e72c4dedafc1cb2d4c5236c9a6957ec7dfd5a9. Image IDs should be sufficiently random so as to be globally unique. 32 bytes read from /dev/urandom is sufficient for all practical purposes. Alternatively, an image ID may be derived as a cryptographic hash of image contents as the result is considered indistinguishable from random. The choice is left up to implementors."

Additional terminology from the Docker Specification.
> "**Image Parent**
Most layer metadata structs contain a parent field which refers to the Image from which another directly descends. An image contains a separate JSON metadata file and set of changes relative to the filesystem of its parent image. Image Ancestor and Image Descendant are also common terms."

> "**Image Checksum**
Layer metadata structs contain a cryptographic hash of the contents of the layer's filesystem changeset. Though the set of changes exists as a simple Tar archive, two archives with identical filenames and content will have different SHA digests if the last-access or last-modified times of any entries differ. For this reason, image checksums are generated using the TarSum algorithm which produces a cryptographic hash of file contents and selected headers only. Details of this algorithm are described in the separate TarSum specification."

> "**Tag**
A tag serves to map a descriptive, user-given name to any single image ID. An image name suffix (the name component after :) is often referred to as a tag as well, though it strictly refers to the full name of an image. Acceptable values for a tag suffix are implementation specific, but they SHOULD be limited to the set of alphanumeric characters [a-zA-z0-9], punctuation characters [._-], and MUST NOT contain a : character."

When a Docker Image is instantiated as a Container in a file system using an "Image Filesystem Changeset". From the specification:
> "An image root filesystem is first created as an empty directory named with the ID of the image being created. Here is the initial empty directory structure for the changeset for an image with ID c3167915dc9d (real IDs are much longer, but this example use a truncated one here for brevity. Implementations need not name the rootfs directory in this way but it may be convenient for keeping record of a large number of image layers.)..."

Files and directories are then created and directory is then committed as a plain Tar archive.

> "The TarSum checksum for the archive file is then computed and placed in the JSON metadata along with the execution parameters. To make changes to the filesystem of this container image, create a new directory named with a new ID, such as f60c56784b83, and initialize it with a snapshot of the parent image's root filesystem, so that the directory is identical to that of c3167915dc9d. NOTE: a copy-on-write or union filesystem can make this very efficient:"

The **CONTAINER ID**, a unique identifier you can use to refer to the container in other commands (this is kind of like a process id in Linux) but only persists while the container is active.


### Existing Metadata in Docker
Docker includes metadata attached to images and containers and may be accessed through the Docker [inspect](https://docs.docker.com/reference/commandline/inspect/) command. Docker inspect returns a JSON array.  

```bash
docker inspect [OPTIONS] CONTAINER|IMAGE [CONTAINER|IMAGE...]

```

Individual fields of the JSON key-value structure may also be searched. For example.
```bash
$ docker inspect --format='{{.NetworkSettings.IPAddress}}' $INSTANCE_ID
```

Metadata is stored by Docker in the */var/lib/docker* directory on the host machine. The exact directory depends on the storage driver.
You can manually set the storage driver with the -s or --storage-driver= option to the Docker daemon.

`/var/lib/docker/{driver-name}` will contain the driver specific storage for contents of the images.
`/var/lib/docker/graph/<id>` now only contains metadata about the image, in the json and layersize files.
In the case of aufs:

`/var/lib/docker/aufs/diff/<id>` has the file contents of the images.
`/var/lib/docker/repositories-aufs` is a JSON file containing local image information. This can be viewed with the command docker images.
In the case of devicemapper:

`/var/lib/docker/devicemapper/devicemapper/data` stores the images
`/var/lib/docker/devicemapper/devicemapper/metadata` the metadata
Note these files are thin provisioned "sparse" files so aren't as big as they seem.

Docker's behavior and output formats may also be modified through the [*config.json*](https://docs.docker.com/reference/commandline/cli/) file that specifies default behavior of the command line as well as formatting options for command results.

Metadata is set through the Docker [label](https://docs.docker.com/userguide/labels-custom-metadata/) *command line option to other commands* that provides a `<key>/<value>` pair. Unfortunately, Docker doesn't provide a direct means of modifying or adding labels to containers or images other than using the run command. For example


```bash
LABEL com.example.image-specs="{\"Description\":\"A containerized foobar\",\"Usage\":\"docker run --rm example\\/foobar [args]\",\"License\":\"GPL\",\"Version\":\"0.0.1-beta\",\"aBoolean\":true,\"aNumber\":0.01234,\"aNestedArray\":[\"a\",\"b\",\"c\"]}"
```
Docker labels may also be specified as a command field in a **Dockerfile** which will be instantiated during the docker [build](https://docs.docker.com/reference/builder/) command. For example:
```
LABEL com.example.label-with-value="foo"
LABEL version="1.0"
LABEL description="This text illustrates \
that label-values can span multiple lines."
```

Docker provides some brief guidelines for constructing labels taken from their documentation as follows:
> To prevent naming conflicts, Docker namespaces label keys using a reverse domain notation. Use the following guidelines to name your keys:

> + All (third-party) tools should prefix their keys with the reverse DNS notation of a domain controlled by the author. For example, com.example.some-label.
> + The com.docker.*, io.docker.* and org.dockerproject.* namespaces are reserved for Docker’s internal use.
> + Keys should only consist of lower-cased alphanumeric characters, dots and dashes (for example, [a-z0-9-.])
> + Keys should start and end with an alpha numeric character
> + Keys may not contain consecutive dots or dashes.
> + Keys without namespace (dots) are reserved for CLI use. This allows end- users to add metadata to their containers and images without having to type cumbersome namespaces on the command-line.

Smart Containers relies on this ability to attach metadata to a container or image. Since a label may be an JSON array, Smart Containers attaches a **JSON-LD** serialization of the graph object to the Docker container. All of the previous provenance history of a Docker Object (image/container) is encoded using RDF/OWL based vocabularies.

### Issues in Docker
Most Linux distributions are not designed to run inside a Docker Container since they require some sort of init system to provide system level services. Docker generally only runs one process withing a container environment, although it has the capability to run multiple tasks. 
> "When your Docker container starts, only the CMD command is run. The only processes that will be running inside the container is the CMD command, and all processes that it spawns. That's why all kinds of important system services are not run automatically – you have to run them yourself."


The [Phusion](http://phusion.github.io/baseimage-docker/) docker base images are attempting to create a set minimal "init" environment based on Ubuntu such that services (databases, connectors, etc) may be run concurrently instead of using the Docker "link" methodology to provide the service by connecting multiple containers. 

## RDF
The Resource Description Framework (RDF) is a framework for representing and linking data within the World Wide Web infrastructure [[RDF1.1](http://www.w3.org/TR/rdf11-concepts/)], and is a graph-based data model, where the core structure of the abstract syntax is a set of triples, each consisting of a subject, a predicate and an object. The idea is that we want relationship identifiers that form a "Knowledge Graph" and represent [things not strings](https://googleblog.blogspot.com/2012/05/introducing-knowledge-graph-things-not.html).
![](https://github.com/charlesvardeman/sc_spec/blob/master/assets/rdf-graph.svg)

There are several serialization syntaxes for storing and exchanging RDF such as [Turtle](http://www.w3.org/TR/turtle/) and [JSON-LD](http://www.w3.org/TR/json-ld/). When creating new RDF resources, Turtle is regarded as a more human *readable* serialization although JSON-LD has the advantage of being . Servers should implement content negotiation in order to handle different serialization formats.


## Linked Data Principles
Tim Berners-Lee published a [technical note](http://www.w3.org/DesignIssues/LinkedData.html) outlining some basic principles for publishing data on the Web. These principles can be summarized as:

1. Use URIs as names for things.
2. Use HTTP URIs so that people can look up those names.
3. When someone looks up a URI, provide useful information, using the standards (RDF*, SPARQL)
4. Include links to other URIs. so that they can discover more things.

Data that adopts some of these principles is referred to as *"Five Star"* linked data. The [W3C](http://www.w3.org/standards/semanticweb/data) continues to create [recommendations](http://www.w3.org/standards/techs/linkeddata#w3c_all) to facilitate the linked data vision. The [BBC](http://www.bbc.co.uk/blogs/radiolabs/s5/linked-data/s5.html) has a nice introduction to linked data principles.
## Ontologies
To facilitate the capturing of *concepts* within RDF data, the W3C has created a specification called the Web Ontology Language (OWL). OWL is on it's second revision and a [primer](http://www.w3.org/TR/owl2-primer/) for OWL 2 is available as a W3C technical report. From this technical report:
> "The W3C OWL 2 Web Ontology Language (OWL) is a Semantic Web language designed to represent rich and complex knowledge about things, groups of things, and relations between things. OWL is a computational logic-based language such that knowledge expressed in OWL can be reasoned with by computer programs either to verify the consistency of that knowledge or to make implicit knowledge explicit. OWL documents, known as ontologies, can be published in the World Wide Web and may refer to or be referred from other OWL ontologies. OWL is part of the W3C's [Semantic Web](http://www.w3.org/2001/sw/) technology stack, which includes RDF [[RDF Concepts]](http://www.w3.org/TR/owl2-primer/#ref-rdf-concepts) and SPARQL [[SPARQL]](http://www.w3.org/TR/owl2-primer/#ref-sparql)."

## Existing Ontologies Leveraged for Smart Containers.
### W3C Provenance Vocabulary
The W3C has created a specification for the exchange of provenance data on the World Wide Web. It has also created an [implementation](http://www.w3.org/TR/2012/CR-prov-o-20121211/) of this specification using OWL 2. From the [Prov Primer](http://www.w3.org/TR/prov-primer/):
> "The provenance of digital objects represents their origins. PROV is a specification to express provenance records, which contain descriptions of the entities and activities involved in producing and delivering or otherwise influencing a given object. Provenance can be used for many purposes, such as understanding how data was collected so it can be meaningfully used, determining ownership and rights over an object, making judgements about information to determine whether to trust it, verifying that the process and steps used to obtain a result complies with given requirements, and reproducing how something was generated."

#### Prov snippit in JSON-LD
An example of Prov in a JSON-LD serialization from the [Kleio](https://github.com/tetherless-world/kleio/tree/master/examples/jsonld-example) github repository. Kleio provides a python implementation of Prov using [Rdflib](https://github.com/RDFLib/rdflib).
```json
{
  "@context": {
    "foaf": "http://xmlns.com/foaf/0.1/",
    "prov": "http://www.w3.org/ns/prov#",
    "rdf": "http://www.w3.org/1999/02/22-rdf-syntax-ns#",
    "rdfs": "http://www.w3.org/2000/01/rdf-schema#",
    "test": "http://tw.rpi.edu/ns/test#",
    "xsd": "http://www.w3.org/2001/XMLSchema#"
  },
  "@graph": [
    {
      "@graph": [
        {
          "@id": "test:bob",
          "@type": "prov:Person",
          "foaf:name": "Bob",
          "prov:influenced": [
            {
              "@id": "test:entity"
            },
            {
              "@id": "test:activity"
            }
          ]
        },
        {
          "@id": "test:entity",
          "@type": "prov:Entity",
          "prov:wasAttributedTo": {
            "@id": "test:bob"
          },
          "prov:wasGeneratedBy": {
            "@id": "test:activity"
          },
          "prov:wasInfluencedBy": [
            {
              "@id": "test:bob"
            },
            {
              "@id": "test:activity"
            }
          ],
          "rdfs:label": "example entity"
        },
        {
          "@id": "test:activity",
          "@type": "prov:Activity",
          "prov:generated": {
            "@id": "test:entity"
          },
          "prov:influenced": {
            "@id": "test:entity"
          },
          "prov:wasAssociatedWith": {
            "@id": "test:bob"
          },
          "prov:wasInfluencedBy": {
            "@id": "test:bob"
          },
          "rdfs:label": "example activity"
        }
      ]
    },
    {
      "@graph": [
        {
          "@id": "test:derived_entity",
          "@type": "prov:Entity",
          "prov:wasDerivedFrom": {
            "@id": "test:entity"
          },
          "prov:wasInfluencedBy": {
            "@id": "test:entity"
          },
          "rdfs:label": "derived example entity"
        },
        {
          "@id": "test:entity",
          "@type": "prov:Entity",
          "prov:influenced": {
            "@id": "test:derived_entity"
          }
        }
      ],
      "@id": "urn:x-rdflib:default"
    }
  ]
}
```
### W3C Health Care and Life Sciences Working Group
The W3C Health Care and Life Sciences have released a [technical note](http://www.w3.org/2001/sw/hcls/notes/hcls-rdf-guide/) on best practices for publishing data as **Linked Data** on the World Wide Web. As part of this groups activities, a series of [recommended](http://www.w3.org/TR/2015/NOTE-hcls-dataset-20150514/) vocabularies for publication of datasets has been established. The vocabularies chosen are a well known set including the previously specified prov vocabulary. The dataset note contains a minimum specification of "**MUSTS**" for a dataset publication to be compliant. Smart Containers should enable the publication of datasets and software within the minimum specification for the vocabulary.

### Open Archives Initiative Object Reuse and Exchange (OAI-ORE)
The [OAI-ORE](https://www.openarchives.org/ore/1.0/primer) defines standards for the description and exchange of aggregations of "web resources" for use in archives and repositories. The Open Archives Initiative (OAI) has released a primer on the use of [JSON-LD](https://www.openarchives.org/ore/0.9/jsonld) for the description of these aggregations of web resources. It should be noted, there is significant overlap between the vocabularies specified by the Health Care and Life Sciences group and the OAI group with ORE only differing by the **Addition** of the ORE (http://www.openarchives.org/ore/terms/) and Fabio (http://purl.org/spar/fabio/) namespaces. A JSON-LD ORE example follows:

```JSON
{ "@context": "https://w3id.org/ore/context",
  "@type": "ResourceMap",
  "describes": {
      "@id": "http://example.com/aggregation-1",
      "@type": "Aggregation",
      
      "aggregates": [
          "http://example.com/document-1",
          { "@id": "http://other.example.org/data-2",
            "@type": "AggregatedResource"
          }
      ]
      
  }
}
```
### General Workflow descriptions
Existing work exists that describe generic scientific workflow processes that can be leveraged for Smart Containers such as the [wfdesc](http://www.sciencedirect.com/science/article/pii/S1570826815000049) vocabulary that uses Prov as a foundational ontology to facilitate interoperability.


### Notes
1) We should follow the ["Five Stars of Linked Data Vocabulary Use"](http://www.semantic-web-journal.net/content/five-stars-linked-data-vocabulary-use) principles.
2) Ontologies should be published as persistent JSON-LD contexts to facilitate extension. 
3) Smart Containers only *Populate* core components of the vocabularies. Most Alignment should happen at the context level.
4) An ontology pattern based approach is will facilitate reuse and interoperability.
5) Links need to be **Dereferencable" to facilitate HATEOS, RESTFul and contextualiztion of resources. The ["Follow your nose principle"](http://patterns.dataincubator.org/book/follow-your-nose.html) is important.

### Ontology Design Patterns created for Smart Containers
 

## Persistent Identifiers and minting URIs
CoolURIs, Trusty URI's WebID ORCID and RDA PID group work.

## Other Relevant Technologies (Linked Data Fragments and Linked Data Platform).


## Libraries and Frameworks for RDF and Semantic Web Development.
